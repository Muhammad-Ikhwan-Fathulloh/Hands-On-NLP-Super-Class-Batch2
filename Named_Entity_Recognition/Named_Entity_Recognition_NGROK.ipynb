{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# Install dependencies\n",
        "!pip install fastapi uvicorn transformers torch pyngrok"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M9z1RE-Mr2pv",
        "outputId": "4ee3819c-a8fa-49dd-ea63-457c1c5a260d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: fastapi in /usr/local/lib/python3.10/dist-packages (0.115.5)\n",
            "Requirement already satisfied: uvicorn in /usr/local/lib/python3.10/dist-packages (0.32.1)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.46.2)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.5.1+cu121)\n",
            "Requirement already satisfied: pyngrok in /usr/local/lib/python3.10/dist-packages (7.2.1)\n",
            "Requirement already satisfied: starlette<0.42.0,>=0.40.0 in /usr/local/lib/python3.10/dist-packages (from fastapi) (0.41.3)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,!=2.1.0,<3.0.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from fastapi) (2.9.2)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from fastapi) (4.12.2)\n",
            "Requirement already satisfied: click>=7.0 in /usr/local/lib/python3.10/dist-packages (from uvicorn) (8.1.7)\n",
            "Requirement already satisfied: h11>=0.8 in /usr/local/lib/python3.10/dist-packages (from uvicorn) (0.14.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.16.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.26.2)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.9.11)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.5)\n",
            "Requirement already satisfied: tokenizers<0.21,>=0.20 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.20.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.6)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2024.10.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,!=2.1.0,<3.0.0,>=1.7.4->fastapi) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.23.4 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,!=2.1.0,<3.0.0,>=1.7.4->fastapi) (2.23.4)\n",
            "Requirement already satisfied: anyio<5,>=3.4.0 in /usr/local/lib/python3.10/dist-packages (from starlette<0.42.0,>=0.40.0->fastapi) (3.7.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.8.30)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.4.0->starlette<0.42.0,>=0.40.0->fastapi) (1.3.1)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.4.0->starlette<0.42.0,>=0.40.0->fastapi) (1.2.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Mount Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MDuiaZXQr5wM",
        "outputId": "c9c69b22-fde4-4a4b-d6ef-d239a749fd45"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v36bpW-9rzFJ"
      },
      "outputs": [],
      "source": [
        "# Import required libraries\n",
        "from fastapi import FastAPI, HTTPException\n",
        "from pydantic import BaseModel\n",
        "from typing import List\n",
        "import torch\n",
        "from torch import nn\n",
        "from transformers import BertTokenizerFast, BertForTokenClassification\n",
        "from pyngrok import ngrok\n",
        "import uvicorn\n",
        "import threading"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define FastAPI app\n",
        "app = FastAPI()\n",
        "\n",
        "# Pre-defined label mappings\n",
        "ids_to_labels = {\n",
        "    0: 'B-art', 1: 'B-eve', 2: 'B-geo', 3: 'B-gpe', 4: 'B-nat',\n",
        "    5: 'B-org', 6: 'B-per', 7: 'B-tim', 8: 'I-art', 9: 'I-eve',\n",
        "    10: 'I-geo', 11: 'I-gpe', 12: 'I-nat', 13: 'I-org', 14: 'I-per',\n",
        "    15: 'I-tim', 16: 'O'\n",
        "}\n",
        "\n",
        "# Define model class\n",
        "class BertModel(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(BertModel, self).__init__()\n",
        "        self.bert = BertForTokenClassification.from_pretrained('bert-base-cased', num_labels=len(ids_to_labels))\n",
        "\n",
        "    def forward(self, input_ids, attention_mask, labels=None):\n",
        "        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask, labels=labels, return_dict=False)\n",
        "        return outputs\n",
        "\n",
        "# Load tokenizer and model\n",
        "tokenizer = BertTokenizerFast.from_pretrained('bert-base-cased')\n",
        "model = BertModel()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kXo7etyXsBOf",
        "outputId": "cb495c5f-badb-4edc-bb9c-4d710478e239"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load model weights from Google Drive\n",
        "model_path = '/content/drive/My Drive/mentoring/Kelas-NLP/models/ner-model.pth'\n",
        "model.load_state_dict(torch.load(model_path, map_location=torch.device('cpu')))\n",
        "model.eval()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PNbHK5PLtZLm",
        "outputId": "b640a5aa-65d7-40fa-9ea8-3f6e5d5c85ae"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-39-fa182f8d624f>:3: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  model.load_state_dict(torch.load(model_path, map_location=torch.device('cpu')))\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "BertModel(\n",
              "  (bert): BertForTokenClassification(\n",
              "    (bert): BertModel(\n",
              "      (embeddings): BertEmbeddings(\n",
              "        (word_embeddings): Embedding(28996, 768, padding_idx=0)\n",
              "        (position_embeddings): Embedding(512, 768)\n",
              "        (token_type_embeddings): Embedding(2, 768)\n",
              "        (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (encoder): BertEncoder(\n",
              "        (layer): ModuleList(\n",
              "          (0-11): 12 x BertLayer(\n",
              "            (attention): BertAttention(\n",
              "              (self): BertSdpaSelfAttention(\n",
              "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "                (dropout): Dropout(p=0.1, inplace=False)\n",
              "              )\n",
              "              (output): BertSelfOutput(\n",
              "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "                (dropout): Dropout(p=0.1, inplace=False)\n",
              "              )\n",
              "            )\n",
              "            (intermediate): BertIntermediate(\n",
              "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "              (intermediate_act_fn): GELUActivation()\n",
              "            )\n",
              "            (output): BertOutput(\n",
              "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (dropout): Dropout(p=0.1, inplace=False)\n",
              "    (classifier): Linear(in_features=768, out_features=17, bias=True)\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Contoh kalimat\n",
        "sentence = \"Barack Obama was the 44th President of the United States.\"\n",
        "\n",
        "# Inisialisasi tokenizer\n",
        "tokenizer = BertTokenizerFast.from_pretrained('bert-base-cased')\n",
        "\n",
        "# Fungsi evaluasi\n",
        "def evaluate_one_text(model, sentence):\n",
        "    text = tokenizer(sentence, padding='max_length', max_length=512, truncation=True, return_tensors=\"pt\")\n",
        "    input_ids = text['input_ids']\n",
        "    attention_mask = text['attention_mask']\n",
        "\n",
        "    with torch.no_grad():\n",
        "        outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
        "\n",
        "    logits = outputs[0]\n",
        "    predictions = torch.argmax(logits, dim=2)\n",
        "\n",
        "    # Konversi ID prediksi ke label\n",
        "    predicted_labels = [ids_to_labels[id] for id in predictions[0].tolist()]\n",
        "    print(predicted_labels)\n",
        "\n",
        "# Panggil fungsi evaluasi\n",
        "evaluate_one_text(model, sentence)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5j7X6oMawSMF",
        "outputId": "03bff665-5756-436d-ec66-db12144efa71"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['O', 'B-per', 'I-per', 'O', 'O', 'O', 'O', 'B-per', 'O', 'O', 'B-geo', 'I-geo', 'O', 'O', 'B-per', 'O', 'O', 'B-per', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-per', 'O', 'O', 'B-per', 'B-per', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-per', 'B-per', 'O', 'O', 'B-per', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-per', 'O', 'O', 'O', 'B-per', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-per', 'B-per', 'O', 'B-per', 'B-per', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-per', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-per', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-per', 'B-per', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-per', 'B-per', 'O', 'O', 'O', 'B-per', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-per', 'O', 'O', 'O', 'O', 'B-per', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-org', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-per', 'O', 'B-per', 'O', 'B-per', 'B-per', 'I-per', 'O', 'O', 'O', 'B-per', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-per', 'O', 'O', 'B-per', 'B-per', 'B-per', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-per', 'B-per', 'O', 'O', 'B-per', 'B-per', 'B-per', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-org', 'O', 'O', 'O', 'B-per', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-org', 'O', 'O', 'O', 'O', 'O', 'B-per', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Pydantic schemas for request and response\n",
        "class TextInput(BaseModel):\n",
        "    text: str\n",
        "\n",
        "class PredictionOutput(BaseModel):\n",
        "    entities: List[str]  # Hanya berisi daftar label entitas\n",
        "\n",
        "# Utility function for entity prediction\n",
        "def predict_entities(text: str):\n",
        "    # Tentukan perangkat (GPU jika ada, jika tidak CPU)\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    model.to(device)\n",
        "\n",
        "    # Tokenisasi input\n",
        "    tokenized = tokenizer(text, padding='max_length', max_length=512, truncation=True, return_tensors=\"pt\")\n",
        "    input_ids = tokenized[\"input_ids\"].to(device)\n",
        "    attention_mask = tokenized[\"attention_mask\"].to(device)\n",
        "\n",
        "    # Gunakan model untuk mendapatkan output\n",
        "    with torch.no_grad():\n",
        "        outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
        "        logits = outputs[0]  # Ambil logits dari output\n",
        "        predictions = torch.argmax(logits, dim=2)  # Ambil argmax di dimensi 2 untuk prediksi per token\n",
        "\n",
        "    # Pastikan prediksi memiliki dimensi yang benar\n",
        "    if predictions.dim() == 1:  # Jika hasilnya menjadi tensor 1D, tambahkan dimensi untuk 2D\n",
        "        predictions = predictions.unsqueeze(0)  # Tambahkan dimensi tambahan di depan\n",
        "\n",
        "    # Ambil word ids dari tokenized input\n",
        "    word_ids = tokenized.word_ids()\n",
        "\n",
        "    # Menghitung label menggunakan fungsi align_word_ids untuk filtering\n",
        "    label_ids = align_word_ids(text)\n",
        "\n",
        "    # Menyaring entitas dari prediksi dengan kata yang terdeteksi\n",
        "    entities = []\n",
        "    previous_word_idx = None\n",
        "    for idx, word_idx in enumerate(word_ids):\n",
        "        # Hanya ambil token yang bukan padding dan bukan subwords\n",
        "        if word_idx is not None and word_idx != previous_word_idx:\n",
        "            label_id = predictions[0][idx].item()  # Ambil nilai dari tensor 2D\n",
        "            # Filter berdasarkan label_ids untuk memilih entitas yang valid\n",
        "            if label_ids[idx] != -100 and ids_to_labels[label_id] != \"O\":  # \"O\" berarti bukan entitas\n",
        "                word = tokenizer.convert_ids_to_tokens([input_ids[0][idx]])[0]  # Dapatkan kata asli dari token\n",
        "                entity = ids_to_labels[label_id]  # Ambil entitas berdasarkan label_id\n",
        "                entities.append(f\"{word}: {entity}\")  # Tambahkan entitas dan kata\n",
        "\n",
        "        previous_word_idx = word_idx  # Update word_idx untuk langkah berikutnya\n",
        "\n",
        "    return entities"
      ],
      "metadata": {
        "id": "fx54gHR2sD8B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def align_word_ids(texts):\n",
        "    # Tokenisasi input\n",
        "    tokenized_inputs = tokenizer(texts, padding='max_length', max_length=512, truncation=True, return_tensors=\"pt\")\n",
        "    word_ids = tokenized_inputs.word_ids()\n",
        "\n",
        "    previous_word_idx = None\n",
        "    label_ids = []\n",
        "\n",
        "    for word_idx in word_ids:\n",
        "        if word_idx is None:\n",
        "            label_ids.append(-100)  # -100 untuk padding atau subwords\n",
        "        elif word_idx != previous_word_idx:\n",
        "            label_ids.append(1)  # Misalnya label untuk entitas (ganti sesuai dengan kasus)\n",
        "        else:\n",
        "            label_ids.append(-100)  # -100 untuk subwords\n",
        "\n",
        "        previous_word_idx = word_idx\n",
        "\n",
        "    return label_ids\n",
        "\n",
        "def evaluate_one_text(model, sentence):\n",
        "    # Menentukan perangkat (GPU jika ada, jika tidak CPU)\n",
        "    use_cuda = torch.cuda.is_available()\n",
        "    device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
        "\n",
        "    if use_cuda:\n",
        "        model = model.cuda()\n",
        "\n",
        "    # Tokenisasi kalimat input\n",
        "    text = tokenizer(sentence, padding='max_length', max_length=512, truncation=True, return_tensors=\"pt\")\n",
        "    mask = text['attention_mask'].to(device)\n",
        "    input_id = text['input_ids'].to(device)\n",
        "\n",
        "    # Menghitung label dari kalimat menggunakan fungsi align_word_ids\n",
        "    label_ids = torch.Tensor(align_word_ids(sentence)).unsqueeze(0).to(device)\n",
        "\n",
        "    # Dapatkan output dari model\n",
        "    with torch.no_grad():\n",
        "        logits = model(input_id, mask)[0]  # Ambil logits output dari model\n",
        "\n",
        "    # Pastikan logits memiliki dimensi yang benar untuk operasi indexing\n",
        "    logits_clean = logits.squeeze(0)[label_ids.squeeze(0) != -100]  # Saring berdasarkan label_ids\n",
        "\n",
        "    # Prediksi dengan argmax pada logits yang sudah bersih\n",
        "    predictions = logits_clean.argmax(dim=1).tolist()  # Ambil argmax dari logits untuk prediksi\n",
        "\n",
        "    # Mengkonversi ID prediksi menjadi label\n",
        "    prediction_label = [ids_to_labels[i] for i in predictions]\n",
        "\n",
        "    # Output hasil\n",
        "    print(f\"Sentence: {sentence}\")\n",
        "    print(f\"Predictions: {prediction_label}\")"
      ],
      "metadata": {
        "id": "vSAtF-0lxiwy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "evaluate_one_text(model, 'Hello World Ikhwan')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vZWNQhvzzxYA",
        "outputId": "2f2f58ac-9ec1-48f8-fa9b-9e5992222f14"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sentence: Hello World Ikhwan\n",
            "Predictions: ['B-org', 'I-org', 'I-org']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "evaluate_one_text(model, 'Bill Gates is the founder of Microsoft')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Uih9iTyd-X3w",
        "outputId": "a64e918d-3855-4f3b-df2b-be7a71851b0e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sentence: Bill Gates is the founder of Microsoft\n",
            "Predictions: ['B-per', 'I-per', 'O', 'O', 'O', 'O', 'B-org']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sentence = \"Hello World Ikhwan\"\n",
        "entities = predict_entities(sentence)\n",
        "print({\"entities\": entities})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zx2cuXdg3P5S",
        "outputId": "37806598-2a8c-4133-afa0-dd2366598dfd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'entities': ['Hello: B-org', 'World: I-org', 'I: I-org']}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sentence = \"Bill Gates is the founder of Microsoft\"\n",
        "entities = predict_entities(sentence)\n",
        "print({\"entities\": entities})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Fhbfe00l_FLY",
        "outputId": "587219e0-6021-44e1-8060-5e453127a82b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'entities': ['Bill: B-per', 'Gates: I-per', 'Microsoft: B-org']}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# FastAPI endpoints\n",
        "@app.get(\"/\")\n",
        "def read_root():\n",
        "    return {\"message\": \"Welcome to the Named Entity Recognition API\"}\n",
        "\n",
        "@app.post(\"/predict\", response_model=PredictionOutput)\n",
        "def predict(text_input: TextInput):\n",
        "    try:\n",
        "        entities = predict_entities(text_input.text)\n",
        "        return {\"entities\": entities}\n",
        "    except Exception as e:\n",
        "        raise HTTPException(status_code=500, detail=str(e))"
      ],
      "metadata": {
        "id": "mAEwtU6qsHxp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!ngrok config add-authtoken YOUR_NGROK_AUTHTOKEN"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8Q9veKhssbFC",
        "outputId": "54a892fb-7f7c-4ade-873a-3a8503db9a7b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Authtoken saved to configuration file: /root/.config/ngrok/ngrok.yml\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to run FastAPI server\n",
        "def run_app():\n",
        "    uvicorn.run(app, host=\"0.0.0.0\", port=8000)\n",
        "\n",
        "# Start ngrok and FastAPI server\n",
        "ngrok_tunnel = ngrok.connect(8000)\n",
        "print(f\"Public URL: {ngrok_tunnel.public_url}\")\n",
        "\n",
        "# Run FastAPI in a thread\n",
        "thread = threading.Thread(target=run_app)\n",
        "thread.start()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L28GadKcsdpR",
        "outputId": "fab0648b-b0aa-487f-c592-38e8a0308674"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Public URL: https://4490-35-236-210-244.ngrok-free.app\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "\n",
        "url = \"https://4490-35-236-210-244.ngrok-free.app/predict\"\n",
        "data = {\"text\": \"Bill Gates is the founder of Microsoft\"}\n",
        "response = requests.post(url, json=data)\n",
        "\n",
        "print(response.json())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6WR3JlD9seoh",
        "outputId": "65c683ed-b606-4d89-9410-f1526a99cdd9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:     35.236.210.244:0 - \"POST /predict HTTP/1.1\" 200 OK\n",
            "{'entities': ['Bill: B-per', 'Gates: I-per', 'Microsoft: B-org']}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "url = \"https://4490-35-236-210-244.ngrok-free.app/predict\"\n",
        "data = {\"text\": \"Hello World Ikhwan\"}\n",
        "response = requests.post(url, json=data)\n",
        "\n",
        "print(response.json())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "htbmguPH9poQ",
        "outputId": "7f2e66ce-2c4b-4286-a63e-50ac70ba6cff"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:     35.236.210.244:0 - \"POST /predict HTTP/1.1\" 200 OK\n",
            "{'entities': ['Hello: B-org', 'World: I-org', 'I: I-org']}\n"
          ]
        }
      ]
    }
  ]
}