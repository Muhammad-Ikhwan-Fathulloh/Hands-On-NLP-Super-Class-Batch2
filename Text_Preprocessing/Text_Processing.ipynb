{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Text Processing\n",
        "Belajar di [Ruby Thalib AI](https://www.rubythalib.ai/)\n",
        "\n",
        "Sumber:\n",
        "*   [Materi](https://drive.google.com/file/d/1qlRbddUOYAe_pXcA7BwxRnK93Qi9vpfU/view)\n",
        "*   [Kode](https://doc.clickup.com/9018060739/p/h/8cr9by3-1658/65f3e8412b13a43)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "LadFDjZvCSXY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Install Libraries\n",
        "!pip install nltk"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YMItCERADTmy",
        "outputId": "265d2232-b447-4043-8de8-27557f6e7825"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.9.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk) (2024.11.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk) (4.67.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HIjUPkTGAezz",
        "outputId": "10b8db09-b2c8-4d23-aed7-b0c542c91b7a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "# Import Libraries\n",
        "import re\n",
        "import nltk\n",
        "nltk.download('punkt_tab')\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
        "from nltk.tokenize import word_tokenize"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Inisialisasi\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "stop_words = set(stopwords.words('indonesian'))\n",
        "stemmer = PorterStemmer()\n",
        "lemmatizer = WordNetLemmatizer()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LMVGsaGLCEpL",
        "outputId": "59a28fb1-41d5-4525-d144-89b470fa2c57"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Teks contoh\n",
        "text = \"Saya belajar Natural Language Processing di rumah.\"\n",
        "print(text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z-luqVDgCIc0",
        "outputId": "17f1bd08-4511-4040-9afa-214c70f9fa6e"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saya belajar Natural Language Processing di rumah.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(text.lower())\n",
        "print(text.upper())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q6wx86RgD9PM",
        "outputId": "633ee873-2bc4-4f4b-d5a0-fafc68028c07"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "saya belajar natural language processing di rumah.\n",
            "SAYA BELAJAR NATURAL LANGUAGE PROCESSING DI RUMAH.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenisasi\n",
        "tokens = word_tokenize(text.lower())  # Mengubah teks ke huruf kecil\n",
        "print(\"Token:\", tokens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3LuN6rXxCKEe",
        "outputId": "b0321c3a-f336-4b1e-9264-5ba5910d4491"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Token: ['saya', 'belajar', 'natural', 'language', 'processing', 'di', 'rumah', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Menghapus Stopwords\n",
        "tokens = [word for word in tokens if word not in stop_words]\n",
        "print(\"Tanpa Stopwords:\", tokens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "83tPA3bBCMkk",
        "outputId": "b4becdf5-c9ff-49e1-ff2a-a2fde0f90787"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tanpa Stopwords: ['belajar', 'natural', 'language', 'processing', 'rumah', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Stemming\n",
        "stemmed = [stemmer.stem(word) for word in tokens]\n",
        "print(\"Hasil Stemming:\", stemmed)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IPxkB41lCOOt",
        "outputId": "6efbbf58-b28d-43f1-e23b-83bd48acc762"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hasil Stemming: ['belajar', 'natur', 'languag', 'process', 'rumah', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Lemmatization\n",
        "lemmatized = [lemmatizer.lemmatize(word) for word in tokens]\n",
        "print(\"Hasil Lemmatization:\", lemmatized)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LGNPlf91CQZe",
        "outputId": "85f5c56c-5b7c-4aa0-bfcf-70e617beb696"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hasil Lemmatization: ['belajar', 'natural', 'language', 'processing', 'rumah', '.']\n"
          ]
        }
      ]
    }
  ]
}